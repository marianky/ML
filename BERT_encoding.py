# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LA9nKg232235mN_OM3iu6AiKTPEvLaOH
"""

from transformers import BertTokenizer, BertForMaskedLM
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
text = ("Napoleon revolutionised military organisation. "
"Napoleon has legacy. "
"Legacy still lives. "
)
rep = tokenizer(text, return_tensors = "pt")

words = ("Napoleon", "revolution", "ised", "military", "organisation", ".",
  "Napoleon", "has", "legacy", ".", "Legacy", "still", "lives", ".")

tokens = []
for t in rep.input_ids[0]:
  tokens.append(int(t))

print()
word_index = 0
for t in tokens:
  if int(t) < 150:
    print(t)
  else:
    print(t, "\t", words[word_index])
    word_index += 1

#BERT learns how words are related by randomly "masking" words, and predicting the masked words from the words around them.

#code to mask words
from transformers import BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction
import torch
from transformers import pipeline


tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
text = ("Napoleon revolutionised military organisation. "
"Napoleon has legacy. "
"Legacy still lives. "
)
rep = tokenizer(text, return_tensors = "pt")

words = ("Napoleon", "revolution", "ised", "military", "organisation", ".",
  "Napoleon", "has", "legacy", ".", "Legacy", "still", "lives", ".")

tokens = []
for t in rep.input_ids[0]:
  tokens.append(int(t))

# Mask 15% of the tokens randomly
rand = torch.rand(rep.input_ids.shape)
mask_arr = (rand < 0.15) * (rep.input_ids != 101) * (rep.input_ids != 102)
selection = torch.flatten(mask_arr[0].nonzero()).tolist()
rep.input_ids[0, selection] = 103  #103 is the mask
after_masking = rep.input_ids

masked_tokens = []
for t in after_masking[0]:
  masked_tokens.append(int(t))

print()
print("Token", "\t", "Masked")
word_index = 0
for (t, a) in zip(tokens, masked_tokens):
  if int(t) < 150:
    print(t)
  else:
    print(t, "\t", a, "\t", words[word_index])
    word_index += 1

unmasker = pipeline('fill-mask', model='bert-base-uncased')
unmasker("Artificial Intelligence [MASK] take over the world.")

#Sentence order is encoded

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')
SentenceA = ("Napoleon has legacy")
SentenceB = ("Legacy still lives")
rep = tokenizer(SentenceA,SentenceB, return_tensors = "pt")

words = ["\t", "[CLS]", "Napoleon has", "legacy", "[SEP]",
  "Legacy", "still", "lives", "[SEP]"]
for w in words:
  print(w, end="\t")
print()

for category in rep:
  print(category, end="\t")
  for r in rep[category]:
    for token in r.tolist():
      print(token, end="\t")
    print()