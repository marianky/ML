# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15bSSwnAek5-UIM-svKe0UK6USwETb8X1
"""

#Making LLM using BLOOM
!pip install transformers
import torch
import transformers
from transformers import BloomForCausalLM
from transformers import BloomTokenizerFast

model = BloomForCausalLM.from_pretrained("bigscience/bloom-560m")
tokenizer = BloomTokenizerFast.from_pretrained("bigscience/bloom-560m")

#Preparing a Prompt
prompt = "Why is the sky blue?"
result_length = len(prompt.split()) +50 # Number of words to add
inputs = tokenizer(prompt, return_tensors="pt")

#A greedy search tries various possible words to add, and always chooses the word with the highest likelihood. There is no randomness.

print(tokenizer.decode(model.generate(inputs["input_ids"],
                       max_length=result_length + 50
                      )[0]))

#Sampling with Top-k + Top-p
#This uses a combination of three methods, allowing an adjustible amount of randomness in word selection.
print(tokenizer.decode(model.generate(inputs["input_ids"],
                       max_length=result_length,
                       do_sample=True,
                       top_k=50,
                       top_p=0.9
                      )[0]))

#beam search, which chooses the most likely word sequence, looking ahead several words.

print(tokenizer.decode(model.generate(inputs["input_ids"],
                       max_length=result_length,
                       num_beams=2,
                       no_repeat_ngram_size=2,
                       early_stopping=True
                      )[0]))